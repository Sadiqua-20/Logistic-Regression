{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xtV5ag4Q5YGz",
        "NlDDVij0vXoE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **THEORETICAL QUESTIONS**"
      ],
      "metadata": {
        "id": "xtV5ag4Q5YGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. What is Logistic Regression, and how does it differ from Linear Regression?"
      ],
      "metadata": {
        "id": "5xXapkLN5jLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 1.\n",
        "\n",
        "**Logistic Regression** is a statistical method used to **predict a binary outcome** — like yes/no, true/false, or 0/1 — based on input data.\n",
        "\n",
        "**Linear Regression** predicts a **continuous numeric value**, like a price, score, or temperature.\n",
        "\n",
        "### Difference:\n",
        "- **Linear Regression**: Output is a number (e.g., predicting a person’s weight).\n",
        "- **Logistic Regression**: Output is a probability → turned into a category (e.g., predicting if a person is diabetic or not).\n",
        "\n",
        "### Example:\n",
        "- Linear: \"What price will the house sell for?\"\n",
        "- Logistic: \"Will the house sell within 30 days: Yes or No?\""
      ],
      "metadata": {
        "id": "qm8Vm4Dq6L0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. What is the mathematical equation of Logistic Regression?"
      ],
      "metadata": {
        "id": "mVnweMuI6nT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 2.\n",
        "\n",
        "- The **mathematical equation of Multivariate Logistic Regression** is:  \n",
        "$ h_\\theta(x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 X_1 + \\theta_2 X_2 + \\dots + \\theta_n X_n)}}$  \n",
        "\n",
        "- The **mathematical equation of Bivariate Logistic Regression** is:  \n",
        "$ h_\\theta(x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 X_1)}}$\n"
      ],
      "metadata": {
        "id": "IF0DfzsW6roL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. Why do we use the Sigmoid function in Logistic Regression?"
      ],
      "metadata": {
        "id": "uq8izUEY8d_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 3.\n",
        "\n",
        "We use the **sigmoid function** in Logistic Regression because it converts any real number(positive or negative) into a probability between 0 and 1.\n",
        "\n",
        "- The output of logistic regression needs to answer:  \n",
        "  \"What’s the chance this belongs to class 1?\"\n",
        "\n",
        "- The raw equation (like in linear regression) can give any number, even negative or over 1 — which doesn’t make sense as a probability.\n",
        "- The **sigmoid function** fixes that by squashing the number into a **range between 0 and 1**.\n",
        "\n",
        "### Visually:\n",
        "- Input: Any number (like -5, 0, 2)\n",
        "- Output after sigmoid: Always between 0 and 1  \n",
        "  (e.g., sigmoid(0) = 0.5, sigmoid(2) ≈ 0.88,  sigmoid(2) ≈ 0.88,  sigmoid(-∞) = 0 sigmoid(∞) = 1)\n",
        "\n",
        "So, we use it to **map predictions to probabilities**, which we can then use to classify (e.g., if prob > 0.5 → class 1, else class 0)."
      ],
      "metadata": {
        "id": "lZAFSyJa8rNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. What is the cost function of Logistic Regression?"
      ],
      "metadata": {
        "id": "NklkkfJh-NBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 4.\n",
        "\n",
        "The **cost function of Logistic Regression** (CF) is called the **Log Loss**.\n",
        "\n",
        "### The formula:\n",
        "\n",
        "$CF $ = $-\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_{i} \\log(h_θ(x_i)) + (1 - y_{i}) \\log(1 - h_θ(x_i)) \\right]$\n",
        "\n",
        "where,\n",
        "-  $n$ : Number of training examples\n",
        "-  $y_i$ : Actual label (0 or 1)\n",
        "- $h_θ(x_i) $: Predicted probability = $ \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 X_1)}}$\n"
      ],
      "metadata": {
        "id": "u_4hu8AE-RDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. What is Regularization in Logistic Regression? Why is it needed?"
      ],
      "metadata": {
        "id": "gvoa3jo7CVi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 5.\n",
        "\n",
        "**Regularization in Logistic Regression** is a technique used to **prevent overfitting** by **penalizing large coefficients** in the model.\n",
        "\n",
        "### Why it's needed:  \n",
        "When the model learns **too much from the training data**, including noise, it may not perform well on the test data. This leads to overfitting.\n",
        "\n",
        "\n",
        "### Three types:\n",
        "- **L1 (Lasso)**: Can make some weights exactly zero and helps with feature selection.\n",
        "- **L2 (Ridge)**: Shrinks weights but doesn’t make them zero and reduces overfitting.\n",
        "- **Elatic Net**: Ridge + Lasso , helps in both reducing overfitting and feature selection.\n"
      ],
      "metadata": {
        "id": "8Afw8JiOCvPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. Explain the difference between Lasso, Ridge, and Elastic Net regression."
      ],
      "metadata": {
        "id": "NdTsxtJ2EPjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 6.\n",
        "\n",
        "### **1. Ridge Regression (L2 Regularization)**  \n",
        "- **Adds a penalty**: Sum of squares of the coefficients:  \n",
        "  Penalty = $\\lambda \\sum \\theta_i^2$\n",
        "- **Effect**: Shrinks coefficients, but does not make them zero  \n",
        "- **Use**: When we need to reduce overfitting\n",
        "\n",
        "\n",
        "### **2. Lasso Regression (L1 Regularization)**  \n",
        "- **Adds a penalty**: Sum of absolute values of coefficients:    \n",
        "  Penalty = $\\lambda \\sum |\\theta_i|$\n",
        "  \n",
        "- **Effect**: Can shrink some coefficients exactly to zero\n",
        "- **Use**: When we want **feature selection** (automatically pick important features)\n",
        "\n",
        "\n",
        "### **3. Elastic Net Regression**  \n",
        "- **Combines L1 and L2 penalties**:  \n",
        "  Penalty = $\\lambda_1 \\sum |\\theta_i| + \\lambda_2 \\sum \\theta_i^2$\n",
        "- **Effect**: Mixes benefits of Ridge and Lasso  \n",
        "- **Use**: We have **many features**, and some are correlated"
      ],
      "metadata": {
        "id": "RwB5zhIBEXT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7. When should we use Elastic Net instead of Lasso or Ridge?"
      ],
      "metadata": {
        "id": "ZD4k_75nKvyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 7.\n",
        "\n",
        "Use **Elastic Net** when we have:\n",
        "1. **Many features** (especially more features than data points)\n",
        "2. **Correlated features** (features that are related to each other)\n",
        "3. **Want both**: feature selection **and** stability(to reduce overfitting)\n"
      ],
      "metadata": {
        "id": "RM3H3HNpK2Of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8. What is the impact of the regularization parameter (λ) in Logistic Regression?"
      ],
      "metadata": {
        "id": "ymbOX_UwLfvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 8.\n",
        "\n",
        "The **regularization parameter $\\lambda$** in Logistic Regression controls **how much penalty** is added for large coefficients.\n",
        "\n",
        "\n",
        "### Impact of $\\lambda$:\n",
        "\n",
        "- **Large  $\\lambda$** (strong regularization):\n",
        "  - Shrinks coefficients more\n",
        "  - Model becomes simpler\n",
        "  - Can lead to underfitting (too simple, misses patterns)\n",
        "\n",
        "- **Small $\\lambda$** (weak regularization):\n",
        "  - Shrinks coefficients less\n",
        "  - Model becomes more flexible\n",
        "  - Can lead to overfitting (too complex, captures noise)"
      ],
      "metadata": {
        "id": "db-1pKwQLmSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9. What are the key assumptions of Logistic Regression?"
      ],
      "metadata": {
        "id": "5IW83tmJRwYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 9.\n",
        "\n",
        "Following are the **key assumptions of Logistic Regression**:\n",
        "\n",
        "### 1. **Binary or Categorical Outcome**\n",
        "- The **dependent variable** should be **binary** (e.g., 0 or 1) or **categorical** for multinomial logistic regression.\n",
        "\n",
        "### 2. **Linearity of Logit**\n",
        "- The **log-odds** (not the outcome itself) must have a **linear relationship** with the independent variables.\n",
        "\n",
        "### 3. **Independence of Observations**\n",
        "- Each observation (row of data) should be **independent** of the others.\n",
        "\n",
        "### 4. **No or Little Multicollinearity**\n",
        "- The **independent variables should not be highly correlated** with each other.\n",
        "\n",
        "### 5. **Large Sample Size**\n",
        "- Logistic regression works best with **a decent amount of data**.\n"
      ],
      "metadata": {
        "id": "iakME0cqR6ST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10. What are some alternatives to Logistic Regression for classification tasks?"
      ],
      "metadata": {
        "id": "YJOBsWiiT2cY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 10.\n",
        "\n",
        "Following are some popular **alternatives to Logistic Regression** for classification tasks:\n",
        "\n",
        "1. **Decision Trees**\n",
        " - Simple, interpretable \"if-else\" models\n",
        " - Handle non-linear relationships well\n",
        "\n",
        "2. **Random Forest**\n",
        " - An ensemble of decision trees\n",
        " - More accurate and robust than a single tree\n",
        "\n",
        "3. **Support Vector Machines (SVM)**\n",
        " - Finds the best boundary between classes\n",
        " - Works well with high-dimensional data\n",
        "\n",
        "\n",
        "4. **Neural Networks**\n",
        " - Powerful for complex patterns\n",
        " - Useful in deep learning and large datasets\n",
        "\n",
        "5. **Naive Bayes**\n",
        " - Based on probability\n",
        " - Fast and works well with text data (e.g., spam detection)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UdbBOlNfUEVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q11. What are Classification Evaluation Metrics?"
      ],
      "metadata": {
        "id": "Nxc-kbhrVlbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 11.\n",
        "\n",
        "Here are the **key classification evaluation metrics**, explained simply:\n",
        "\n",
        "### 1. **Confusion Matrix**\n",
        "- A table showing:\n",
        "  - True Positives (TP)\n",
        "  - False Positives (FP)\n",
        "  - True Negatives (TN)\n",
        "  - False Negatives (FN)  \n",
        "- Helps visualize performance\n",
        "\n",
        "### 2. **Accuracy**\n",
        "- **What it tells**: How many predictions were correct overall  \n",
        "- **Formula**:  \n",
        "  \n",
        "  $\\text{Accuracy} = \\frac{\\text{Correct predictions}}{\\text{Total predictions}} = \\frac{TP + TN}{TP +FP + FN + TN}$\n",
        "  \n",
        "- Good for balanced datasets  \n",
        "- Not reliable if classes are imbalanced\n",
        "\n",
        "\n",
        "### 3. **Precision**\n",
        "- **What it tells**: Out of all predicted positives, how many are predicted correct  \n",
        "- **Formula**:  \n",
        "  $\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$\n",
        "\n",
        "### 4. **Recall (Sensitivity)**\n",
        "- **What it tells**: Out of all actual positives(1), how many are actually predicted 1.\n",
        "- **Formula**:  \n",
        "  \n",
        "  $\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$\n",
        "\n",
        "\n",
        "### 5. **$F_\\beta$ -Score**\n",
        "- **What it tells**: Balance between precision and recall  \n",
        "- **Formula**:  \n",
        "  $F_\\beta Score = (1+ \\beta^2) \\times (\\frac{\\text{Precision × Recall}}{\\text{Precision + Recall}})$\n",
        "- Useful when we need both precision & recall\n",
        "\n",
        "### 6. **False Positive Rate**\n",
        "- **What it tells**: Out of all actual 0, how many are predicted 1\n",
        "- **Formula**:  \n",
        "  $\\text{FPR} = \\frac{\\text{False Positives}}{\\text{False Positives + True Negatives }}$\n",
        "\n",
        "### 7. **True Negative Rate(SPECIFICITY)**\n",
        "- **What it tells**: Out of all actual 0, how many are actually predicted 0\n",
        "- **Formula**:  \n",
        "  $\\text{TNR} = \\frac{\\text{True Negatives}}{\\text{True Negatives + False Positives}} = 1 - FPR$\n",
        "\n",
        "### 8. **ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**\n",
        "- **AUC** tells how well the model separates classes  \n",
        "- 1 = perfect, 0.5 = random guessing\n",
        "- Higher the AUR better the model will be"
      ],
      "metadata": {
        "id": "Kfb9iDE4Vtwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q12. How does class imbalance affect Logistic Regression?"
      ],
      "metadata": {
        "id": "2uFIMfuFeAHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 12.\n",
        "\n",
        "**Class imbalance** affects **Logistic Regression** by making it **biased toward the majority class**.\n",
        "\n",
        "- Logistic Regression tries to **minimize overall error**.\n",
        "- If 95% of our data is class 0 and only 5% is class 1, the model may just predict everything as class 0 and still get 95% accuracy!\n",
        "\n",
        "### Effects:\n",
        "1. High accuracy, poor performance on minority class  \n",
        "2. Low recall (sensitivity) for the rare class (e.g., missing fraud or disease cases)\n"
      ],
      "metadata": {
        "id": "qiqTZwGneGz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q13. What is Hyperparameter Tuning in Logistic Regression?"
      ],
      "metadata": {
        "id": "cmt6D14LfhIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 13.\n",
        "\n",
        "**Hyperparameter tuning** in Logistic Regression means **adjusting settings** that control how the model learns, to improve performance.\n",
        "\n",
        "###  Key Hyperparameters:\n",
        "1. **Regularization strength (C)**  \n",
        "   - Inverse of $\\lambda (C = 1/ \\lambda$)\n",
        "   - Smaller C = stronger regularization (simpler model)\n",
        "\n",
        "2. **Penalty type**  \n",
        "   - `'l1'` (Lasso), `'l2'` (Ridge), or `'elasticnet'`\n",
        "\n",
        "3. **Solver**  \n",
        "   - Optimization algorithm (e.g., `'liblinear'`, `'saga'`)\n",
        "\n",
        "\n",
        "###  Why tune them?\n",
        "- To find the best combo that gives **high accuracy**, **good generalization**, and **low overfitting**.\n",
        "\n",
        "\n",
        "###  How to tune:\n",
        "- Use methods like **Grid Search** or **Random Search**  \n",
        "- Often combined with **Cross-Validation**"
      ],
      "metadata": {
        "id": "yEQ_Pt21f60i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q14. What are different solvers in Logistic Regression? Which one should be used?"
      ],
      "metadata": {
        "id": "iSTgSyCPhX6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 14.\n",
        "\n",
        "Different solvers in Logistic Regression are:\n",
        "\n",
        "1. **liblinear**\n",
        "\n",
        "2. **saga**\n",
        "\n",
        "3. **lbfgs**\n",
        "\n",
        "4. **newton-cg**\n",
        "\n",
        "5. **sag**\n",
        "\n",
        "\n",
        "### Which solver should we use?\n",
        "- **For small datasets or L1 regularization**:  **`liblinear`**.\n",
        "- **For large datasets or Elastic Net**:  **`saga`**.\n",
        "- **For medium to large datasets with L2 regularization**:  **`lbfgs`**.\n",
        "- **For multinomial classification**:  **`newton-cg`**.\n",
        "- **For very large datasets with L2 regularization**:  **`sag`**.\n",
        "\n",
        "In general, **`lbfgs`** and **`saga`** are the most commonly used solvers."
      ],
      "metadata": {
        "id": "eR516-1Qhj4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q15.  How is Logistic Regression extended for multiclass classification?"
      ],
      "metadata": {
        "id": "Hz3t_vBtj-G9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 15.\n",
        "\n",
        "**Logistic Regression** can be extended for **multiclass classification** using two main strategies:\n",
        "\n",
        "### 1. **One-vs-Rest (OvR)**\n",
        "- **Idea**: For \\( K \\) classes, train \\( K \\) binary classifiers, each one distinguishing a specific class from the rest.\n",
        "- **Process**:\n",
        "  - Each classifier predicts whether the instance belongs to the \"class of interest\" (1) or not (0).\n",
        "  - At prediction time, the model with the highest probability decides the class.\n",
        "  \n",
        "  **Example**: For a 3-class problem (A, B, C),  we'd train 3 classifiers:\n",
        "  - Classifier 1: A vs. (B, C)\n",
        "  - Classifier 2: B vs. (A, C)\n",
        "  - Classifier 3: C vs. (A, B)\n",
        "  \n",
        "  When predicting, the classifier that gives the highest probability chooses the class.\n",
        "\n",
        "### 2. **Multinomial (Softmax) Logistic Regression**\n",
        "- Multinomial Logistic Regression directly handles multiclass classification by using the softmax function to predict probabilities for each class in a single step. The model computes one set of probabilities for all classes, and the class with the highest probability is chosen.\n",
        "  \n",
        "  **Example**: In a 3-class classification problem, instead of computing separate binary decisions, the model computes a set of probabilities for all 3 classes, and the class with the highest probability is chosen."
      ],
      "metadata": {
        "id": "5PWJwHd0kHJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q16. What are the advantages and disadvantages of Logistic Regression?"
      ],
      "metadata": {
        "id": "hgEhzJ1El5tQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 16.\n",
        "\n",
        "### **Advantages of Logistic Regression**:\n",
        "\n",
        "1. **Simplicity & Interpretability**:\n",
        "   - Easy to understand and interpret.\n",
        "   - Coefficients can show the relationship between input features and the outcome.\n",
        "\n",
        "2. **Fast to Train**: Computationally efficient, works well on small to medium datasets.\n",
        "\n",
        "3. **Probabilistic Output**: Provides probabilities (not just class labels), which can be useful in decision-making processes.\n",
        "\n",
        "4. **Less Prone to Overfitting**: With regularization (L1, L2), it can handle overfitting, especially for simpler models.\n",
        "\n",
        "\n",
        "### **Disadvantages of Logistic Regression**:\n",
        "\n",
        "1. **Assumes Linearity**: Assumes a **linear relationship** between input features and the log-odds of the outcome, which may not hold for complex data.\n",
        "\n",
        "2. **Sensitive to Outliers**: Logistic regression can be **sensitive to outliers**, as outliers can disproportionately affect model coefficients.\n",
        "\n",
        "3. **Requires Feature Engineering**: It assumes that the features are linearly related to the target, so complex features may require transformation (e.g., polynomial features) for better results.\n",
        "\n",
        "4. **Imbalanced Data Issues**: In case of highly imbalanced classes, logistic regression may be biased toward the majority class unless adjustments (like class weighting) are made."
      ],
      "metadata": {
        "id": "TnZnaMSDmBB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q17. What are some use cases of Logistic Regression?"
      ],
      "metadata": {
        "id": "8w6NU_AToZjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 17.\n",
        "\n",
        "**Logistic Regression** is widely used in various fields for **binary classification** tasks, where the goal is to predict one of two possible outcomes:\n",
        "\n",
        "### 1. **Medical Diagnosis**\n",
        "   - **Example**: Predicting whether a patient has a particular disease (e.g., cancer: yes/no).\n",
        "  \n",
        "### 2. **Credit Scoring**\n",
        "   - **Example**: Predicting whether a borrower will default on a loan (yes/no).\n",
        "\n",
        "### 3. **Email Spam Detection**\n",
        "   - **Example**: Classifying emails as spam or not spam.\n",
        "\n",
        "### 4. **Customer Churn Prediction**\n",
        "   - **Example**: Predicting whether a customer will leave a service (e.g., telecom, subscription services).\n",
        "   - **Why**: Helps businesses identify at-risk customers and take action to retain them.\n",
        "\n",
        "### 5. **Marketing Campaign Effectiveness**\n",
        "   - **Example**: Predicting whether a customer will respond to a marketing campaign (yes/no)."
      ],
      "metadata": {
        "id": "b8CtwCV5ogTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q18. What is the difference between Softmax Regression and Logistic Regression?"
      ],
      "metadata": {
        "id": "AM1TDrVXpv2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 18.\n",
        "\n",
        "###  **Difference between Softmax Regression and Logistic Regression**:\n",
        "\n",
        "\n",
        "### **1. Logistic Regression**  \n",
        "- **Used for**: **Binary classification** (2 classes: e.g., yes/no, 0/1)  \n",
        "- **Activation function**: **Sigmoid**  \n",
        "- **Output**: Probability of one class (e.g., class 1), then threshold (e.g., 0.5) to decide\n",
        "\n",
        "\n",
        "### **2. Softmax Regression (Multinomial Logistic Regression)**  \n",
        "- **Used for**: **Multiclass classification** (3 or more classes)  \n",
        "- **Activation function**: **Softmax**  \n",
        "- **Output**: Probability distribution over **all classes**, picks the class with the highest probability\n"
      ],
      "metadata": {
        "id": "Z1FAT6frp4gI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?"
      ],
      "metadata": {
        "id": "ITU1TG_ZqrnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 19.\n",
        "\n",
        "\n",
        "###  **Use One-vs-Rest (OvR)** when:\n",
        "- We have a **large number of classes**\n",
        "- We want **simplicity** or faster training\n",
        "- We’re using models that **only support binary classification**\n",
        "\n",
        "\n",
        "###  **Use Softmax (Multinomial Logistic Regression)** when:\n",
        "- We want a **single model** that handles all classes at once\n",
        "- Our classes are **mutually exclusive**\n",
        "- We prefer a **more mathematically elegant** solution with better **probability estimates**"
      ],
      "metadata": {
        "id": "ddagEhEiqzlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q20. How do we interpret coefficients in Logistic Regression?"
      ],
      "metadata": {
        "id": "kXJUoLIOsTDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANS 20.\n",
        "\n",
        "\n",
        "###  Step-by-step Interpretation:\n",
        "\n",
        "1. **Raw Coefficient ($\\beta$)**:\n",
        "   - Shows how much the **log-odds** change with a 1-unit increase in the feature, **holding other features constant**.\n",
        "   - Positive  $\\beta$: Increases the log-odds → higher chance of class 1  \n",
        "   - Negative  $\\beta$ : Decreases the log-odds → lower chance of class 1\n",
        "\n",
        "2. **Exponentiated Coefficient**  $e^{\\beta}$ :  \n",
        "   - Converts log-odds change into an **odds ratio**\n",
        "   - If  $e^{\\beta} > 1$ : Feature **increases** the odds of class 1  \n",
        "   - If  $e^{\\beta} < 1$ : Feature **decreases** the odds of class 1  \n",
        "   - If  $e^{\\beta} = 1$ : No effect\n",
        "\n",
        "\n",
        "###  Example:\n",
        "If a coefficient for \"age\" is 0.4:\n",
        "- Log-odds increase by 0.4 for each 1-year increase in age.\n",
        "- Odds of class 1 increase by $e^{0.4} ≈ 1.49$ , or **49% higher odds** per year."
      ],
      "metadata": {
        "id": "_UO_VWK8sbqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PRACTICAL QUESTIONS**"
      ],
      "metadata": {
        "id": "NlDDVij0vXoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "40CRAQiCxog7"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy."
      ],
      "metadata": {
        "id": "JyPiD5NQvf28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression Model Accuracy:\", round(accuracy * 100, 2), \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9Eiv5jGv3WO",
        "outputId": "c144cc5a-a402-46c3-a06d-a2fbcf8553f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 97.78 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy."
      ],
      "metadata": {
        "id": "4ADMNv3nw7o9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression with L1 Regularization Accuracy:\", round(accuracy * 100, 2), \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF_fsb7yxHCt",
        "outputId": "3b34649b-1175-4e04-f05e-cc7951a7f4dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with L1 Regularization Accuracy: 93.33 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients."
      ],
      "metadata": {
        "id": "hV3-hFvly-sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, multi_class='auto')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression with L2 Regularization Accuracy:\", round(accuracy * 100, 2), \"%\")\n",
        "\n",
        "print(\"\\nModel Coefficients:\")\n",
        "for i, coef in enumerate(model.coef_):\n",
        "    print(f\"Class {i}: {coef}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7wbyBnyzJ8q",
        "outputId": "6f572b08-25a6-400f-9d62-62010d866a80"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with L2 Regularization Accuracy: 97.78 %\n",
            "\n",
            "Model Coefficients:\n",
            "Class 0: [-0.4346298   0.76021294 -2.24087351 -0.93260619]\n",
            "Class 1: [ 0.57993789 -0.53810954 -0.1299831  -0.79998699]\n",
            "Class 2: [-0.14530809 -0.2221034   2.37085661  1.73259318]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')."
      ],
      "metadata": {
        "id": "ayYDfbTI0mHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression with Elastic Net Regularization Accuracy:\", round(accuracy * 100, 2), \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLUSCElp0syz",
        "outputId": "4dc1a5ae-f70d-41cf-cba2-ac7ae6140717"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with Elastic Net Regularization Accuracy: 97.78 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'."
      ],
      "metadata": {
        "id": "kIbdg91l2bDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Multiclass Logistic Regression (OvR) Accuracy:\", round(accuracy * 100, 2), \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvZnnHv72hZg",
        "outputId": "d1dea876-6b13-4158-98bc-ab8683a0e78e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass Logistic Regression (OvR) Accuracy: 88.89 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "1cba_Ji3tyaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "logreg = LogisticRegression(solver='liblinear', multi_class='auto')\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Test set accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKD1xI8Qt8Oz",
        "outputId": "41f0d6a2-2670-4bfd-c012-17056c4c2e4d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Test set accuracy: 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy."
      ],
      "metadata": {
        "id": "yeC8XvtRyV_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "model = LogisticRegression(solver='liblinear', multi_class='auto')\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "print(\"Accuracies for each fold:\", scores)\n",
        "print(\"Average accuracy:\", np.mean(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9REPMvF8yhjI",
        "outputId": "36c58f74-907b-4427-cca3-27ac7f0bf405"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies for each fold: [0.96666667 0.96666667 1.         0.86666667 0.93333333]\n",
            "Average accuracy: 0.9466666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy."
      ],
      "metadata": {
        "id": "DSANxn43zr-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('dataset.csv')\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on test set:\", accuracy)"
      ],
      "metadata": {
        "id": "-Wcy2Qr5285j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "7KgoV88f2-ST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "params = {'C': np.logspace(-4, 4, 10), 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga']}\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "search = RandomizedSearchCV(model, params, n_iter=10, cv=5, scoring='accuracy', random_state=1)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", search.best_params_)\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, search.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lbea_DH7y3C",
        "outputId": "83c6e9bf-b0d8-4b36-c3a7-bd97e1b10e2b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'solver': 'saga', 'penalty': 'l1', 'C': np.float64(2.782559402207126)}\n",
            "Test accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "ki8wnu4_8guG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "model = OneVsOneClassifier(LogisticRegression(solver='liblinear'))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"One-vs-One Logistic Regression accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtQ_eNqT8nZ2",
        "outputId": "7e024ff4-7667-42d3-a930-8f18a64117f8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification."
      ],
      "metadata": {
        "id": "eXH9fgTg9LiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "Data = load_breast_cancer()\n",
        "X = Data.data\n",
        "y = Data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Pred 0\", \"Pred 1\"], yticklabels=[\"Actual 0\", \"Actual 1\"])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "3HMql2JZ9aQj",
        "outputId": "a3954987-4d4c-4f01-f546-8af8389d5971"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n"
      ],
      "metadata": {
        "id": "GDest4gc_QYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "Data = load_breast_cancer()\n",
        "X = Data.data\n",
        "y = Data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27HpME5Q_ZwP",
        "outputId": "c083e9a5-9e4c-4724-b24d-a043a9f09236"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.935064935064935\n",
            "Recall: 1.0\n",
            "F1-Score: 0.9664429530201343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance."
      ],
      "metadata": {
        "id": "zbAnmu8LABhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], flip_y=0, random_state=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model_no_weights = LogisticRegression(max_iter=1000)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "print(\"Without class weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "model_with_weights = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model_with_weights.fit(X_train, y_train)\n",
        "y_pred_with_weights = model_with_weights.predict(X_test)\n",
        "\n",
        "print(\"With class weights:\")\n",
        "print(classification_report(y_test, y_pred_with_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwJGm-xDAK7l",
        "outputId": "8d58bb51-6e62-4eb0-d714-28cab07ffe98"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96       181\n",
            "           1       0.62      0.53      0.57        19\n",
            "\n",
            "    accuracy                           0.93       200\n",
            "   macro avg       0.79      0.75      0.77       200\n",
            "weighted avg       0.92      0.93      0.92       200\n",
            "\n",
            "With class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.85      0.91       181\n",
            "           1       0.37      0.84      0.52        19\n",
            "\n",
            "    accuracy                           0.85       200\n",
            "   macro avg       0.68      0.85      0.71       200\n",
            "weighted avg       0.92      0.85      0.87       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance."
      ],
      "metadata": {
        "id": "6ZgpqyvfKf4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Dropping columns with too many missing or irrelevant information\n",
        "df = df.drop(columns=['deck', 'embark_town', 'alive', 'who', 'adult_male', 'class'])\n",
        "\n",
        "# Dropping rows with missing target\n",
        "df = df.dropna(subset=['survived'])\n",
        "\n",
        "# Filling missing values\n",
        "df['age'] = df['age'].fillna(df['age'].median())\n",
        "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoders = {}\n",
        "for col in ['sex', 'embarked']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "X = df.drop(columns='survived')\n",
        "y = df['survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Performance on Titanic Dataset:\\n\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wyuo9kxLAYQ",
        "outputId": "f007ef14-d2f6-4bbd-8683-61c03432eac5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance on Titanic Dataset:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       106\n",
            "           1       0.77      0.70      0.73        73\n",
            "\n",
            "    accuracy                           0.79       179\n",
            "   macro avg       0.79      0.78      0.78       179\n",
            "weighted avg       0.79      0.79      0.79       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n"
      ],
      "metadata": {
        "id": "tpMISJ2MY_oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "Data = load_breast_cancer()\n",
        "X = Data.data\n",
        "y = Data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Without Scaling\n",
        "model1 = LogisticRegression(max_iter=1000)\n",
        "model1.fit(X_train, y_train)\n",
        "pred1 = model1.predict(X_test)\n",
        "acc1 = accuracy_score(y_test, pred1)\n",
        "\n",
        "# With Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model2 = LogisticRegression(max_iter=1000)\n",
        "model2.fit(X_train_scaled, y_train)\n",
        "pred2 = model2.predict(X_test_scaled)\n",
        "acc2 = accuracy_score(y_test, pred2)\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc1)\n",
        "print(\"Accuracy with scaling   :\", acc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EigMAGwRZOQR",
        "outputId": "66b9c907-e5da-4bec-ff64-d8db3b03d412"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.956140350877193\n",
            "Accuracy with scaling   : 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "Pq6G_pP0aAd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "Data = load_breast_cancer()\n",
        "X = Data.data\n",
        "y = Data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD7S7_6nZ_ql",
        "outputId": "b2e2da8e-ec24-490b-e82e-8c8a8fc8f05f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9877645502645502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy."
      ],
      "metadata": {
        "id": "AiLaPMUhhgok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "Data = load_breast_cancer()\n",
        "X = Data.data\n",
        "y = Data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "model = LogisticRegression(C=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy with C=0.5:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyZaE1Ithpqc",
        "outputId": "ee5da19c-e3f0-4ee9-acee-958e953f14ed"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q18. Write a Python program to train Logistic Regression and identify important features based on model coefficients."
      ],
      "metadata": {
        "id": "qwEyuQ_0jhNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "Data = load_breast_cancer()\n",
        "X = Data.data\n",
        "y = Data.target\n",
        "feature_names = Data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "coefficients = model.coef_[0]\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients, 'Abs_Coefficient': abs(coefficients)})\n",
        "\n",
        "feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"Important Features Based on Model Coefficients:\")\n",
        "print(feature_importance[['Feature', 'Coefficient']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJjCEErTjnqY",
        "outputId": "a3d980b0-6191-4bfc-a2f2-b508e833cb34"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Features Based on Model Coefficients:\n",
            "                    Feature  Coefficient\n",
            "20             worst radius     1.254037\n",
            "0               mean radius     1.086279\n",
            "26          worst concavity    -0.725623\n",
            "25        worst compactness    -0.553948\n",
            "21            worst texture    -0.425673\n",
            "1              mean texture     0.425324\n",
            "6            mean concavity    -0.273270\n",
            "11            texture error     0.249824\n",
            "12          perimeter error     0.228840\n",
            "27     worst concave points    -0.213818\n",
            "22          worst perimeter    -0.196498\n",
            "5          mean compactness    -0.190369\n",
            "28           worst symmetry    -0.151297\n",
            "7       mean concave points    -0.111770\n",
            "2            mean perimeter     0.107302\n",
            "13               area error    -0.076718\n",
            "16          concavity error    -0.065987\n",
            "10             radius error     0.065107\n",
            "24         worst smoothness    -0.060616\n",
            "8             mean symmetry    -0.053315\n",
            "29  worst fractal dimension    -0.044613\n",
            "15        compactness error    -0.043955\n",
            "4           mean smoothness    -0.035285\n",
            "23               worst area    -0.017914\n",
            "18           symmetry error    -0.016731\n",
            "17     concave points error    -0.016487\n",
            "9    mean fractal dimension    -0.009007\n",
            "3                 mean area    -0.007816\n",
            "14         smoothness error    -0.004142\n",
            "19  fractal dimension error    -0.004014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score."
      ],
      "metadata": {
        "id": "15hul0hCkg_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "Data = load_breast_cancer()\n",
        "X = Data.data\n",
        "y = Data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen's Kappa Score:\", kappa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyOP-lN2kpIO",
        "outputId": "27d3e313-8385-4228-865e-e79652fcc15a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.8857715430861723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification."
      ],
      "metadata": {
        "id": "1SfU0IQ5lUAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "Data = load_breast_cancer()\n",
        "X = Data.data\n",
        "y = Data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, label=f'Avg Precision = {avg_precision:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "GyDuDDzBlr4t",
        "outputId": "8240b2d2-43fa-4b80-b443-76786b77064a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy."
      ],
      "metadata": {
        "id": "j1oedW6ko3xK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "Data = load_breast_cancer()\n",
        "X = Data.data\n",
        "y = Data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "accuracy_results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracy_results[solver] = acc\n",
        "    print(f\"Accuracy with solver='{solver}': {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ8-fcxTpmGO",
        "outputId": "8ca1a278-d062-438b-a974-78e60eface37"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with solver='liblinear': 0.9561\n",
            "Accuracy with solver='saga': 0.8772\n",
            "Accuracy with solver='lbfgs': 0.9474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n"
      ],
      "metadata": {
        "id": "577uIaGwqB3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "Data = load_breast_cancer()\n",
        "X = Data.data\n",
        "y = Data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-4cBeWWrkxq",
        "outputId": "bf7e4f76-87c0-4b4a-9058-6ca1522bf695"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.8864143349643965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "q0qUg4ICr6GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "Data = load_breast_cancer()\n",
        "X = Data.data\n",
        "y = Data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Logistic Regression on Raw Data\n",
        "model_raw = LogisticRegression()\n",
        "model_raw.fit(X_train, y_train)\n",
        "pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, pred_raw)\n",
        "\n",
        "# Logistic Regression on Standardized Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression()\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, pred_scaled)\n",
        "\n",
        "print(\"Accuracy on raw data       :\", acc_raw)\n",
        "print(\"Accuracy on standardized data:\", acc_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIvXAHmqsydi",
        "outputId": "07a9303e-b557-4591-c436-d3a325950e72"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data       : 0.9473684210526315\n",
            "Accuracy on standardized data: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation."
      ],
      "metadata": {
        "id": "xRF_x1FBvZba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "Data = load_breast_cancer()\n",
        "X = Data.data\n",
        "y = Data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "param_grid = {'C': np.logspace(-4, 4, 10)}\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best C value:\", grid.best_params_['C'])\n",
        "print(\"Best cross-validation accuracy:\", grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3gveuBbvruL",
        "outputId": "0aa26345-a9e8-46a2-d1ea-8149c7e9c749"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C value: 21.54434690031882\n",
            "Best cross-validation accuracy: 0.9472527472527472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions."
      ],
      "metadata": {
        "id": "uW9HPiQOy673"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "Data = load_breast_cancer()\n",
        "X = Data.data\n",
        "y = Data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "import joblib\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "print(\"Accuracy of loaded model:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8WFR0l9zRXR",
        "outputId": "3b48e496-fd6f-437c-dfa0-43b7647a312a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of loaded model: 0.9473684210526315\n"
          ]
        }
      ]
    }
  ]
}